{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import itertools\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torchvision.transforms import ToTensor, Normalize, ToPILImage\n",
        "from torchvision.transforms.functional import hflip, vflip, rotate, adjust_hue\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from torchsummary import summary\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "MtzFZqyBTur6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key Parameters \n",
        "download_original= 0 # download the slide image and masks from google bucket\n",
        "generate_data = 1 # generate the downscale data\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# original width and height of image (standardized in all images)\n",
        "width, height = 178, 218"
      ],
      "metadata": {
        "id": "-rYBeGY8T39s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGKDsSWl_H48"
      },
      "outputs": [],
      "source": [
        "# download the image the google bucket that I set up \n",
        "# credit: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
        "if download_original == 1: \n",
        "    image_url = 'https://storage.googleapis.com/acv_project/celeb_img.zip'\n",
        "    !curl -O $image_url\n",
        "    !unzip celeb_img\n",
        "    !rm celeb_img.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if generate_data == 1: \n",
        "    # train test split\n",
        "    # split by id \n",
        "    img_id = os.listdir('./img_align_celeba')[:100000]\n",
        "    train_id, test_id = train_test_split(img_id, test_size=0.1, random_state=1)      \n",
        "\n",
        "    # create test and train folder    \n",
        "    !rm -rf ./train\n",
        "    os.mkdir('./train') \n",
        "    os.mkdir('./train/original') # save all train data to this path\n",
        "\n",
        "    !rm -rf ./test\n",
        "    os.mkdir('./test') \n",
        "    os.mkdir('./test/original') # save all train data to this path\n",
        "\n",
        "    # save img into the respective folders as 1x downsampling\n",
        "    for id in train_id: \n",
        "        shutil.copy('img_align_celeba/' + id, './train/original')\n",
        "\n",
        "    for id in test_id: \n",
        "        shutil.copy('img_align_celeba/' + id, './test/original')\n",
        "\n",
        "    # downscale images via Pillow \n",
        "    # this will take quite some time to run\n",
        "    downscale_factor_list = [1, 2, 4]\n",
        "    for path in ['./train/', './test/']:\n",
        "        img_id_list = os.listdir(path + 'original/')\n",
        "        for img_id in tqdm(img_id_list): \n",
        "            img = Image.open(path + 'original/' + img_id)\n",
        "            img_arr_1x = np.array(img) # 1x downscale\n",
        "            # downscale and upscale again (bicubic method)\n",
        "            newsize_2x = (int(width/2), int(height/2))\n",
        "            img_2x = img.resize(newsize_2x) \n",
        "            img_2x = img_2x.resize((width, height))\n",
        "            img_arr_2x = np.array(img_2x) # 2x downscale\n",
        "            newsize_4x = (int(width/4), int(height/4))\n",
        "            img_4x = img.resize(newsize_4x) \n",
        "            img_4x = img_4x.resize((width, height))\n",
        "            img_arr_4x = np.array(img_4x) # 4x downscale\n",
        "            np.save(os.path.join(path, img_id[:-4]), np.stack((img_arr_1x, img_arr_2x, img_arr_4x)))\n",
        "    \n",
        "    !rm -rf ./train/original\n",
        "    !rm -rf ./test/original"
      ],
      "metadata": {
        "id": "6PeXBPjuUHD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset and dataloader\n",
        "class CelebDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_files = os.listdir(img_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # load the image from disk \n",
        "        img_arr = np.load(os.path.join(self.img_dir, self.img_files[idx]))\n",
        "        img_1x = Image.fromarray(img_arr[0])\n",
        "        img_2x = Image.fromarray(img_arr[1])\n",
        "        img_4x = Image.fromarray(img_arr[2])\n",
        "\n",
        "        # apply flip and convert to tensor \n",
        "        # flipping\n",
        "        c = np.random.rand()\n",
        "        if c > 0.5: \n",
        "            img_1x = hflip(img_1x)\n",
        "            img_2x = hflip(img_2x)\n",
        "            img_4x = hflip(img_4x)\n",
        "\n",
        "        # to tensor \n",
        "        img_1x = ToTensor()(img_1x)\n",
        "        img_2x = ToTensor()(img_2x)\n",
        "        img_4x = ToTensor()(img_4x)\n",
        "\n",
        "        return torch.stack([img_1x, img_2x, img_4x])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Filter failed images first\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "    \n",
        "    # Now collate into mini-batches\n",
        "    img = torch.stack([b for b in batch]) \n",
        "    return img\n",
        "\n",
        "# implement custom image_dataset and wrap it with the dataloader\n",
        "image_datasets = {x: CelebDataset(os.path.join('./', x)) for x in ['train', 'test']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, \n",
        "                                              shuffle=True, num_workers=0, collate_fn = collate_fn)\n",
        "              for x in ['train', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "print('size of dataset', dataset_sizes)"
      ],
      "metadata": {
        "id": "EllILXvflUNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample output\n",
        "img = next(iter(dataloaders['test']))[0]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "# Display the LR and HR images using matplotlib\n",
        "axs[0].imshow(ToPILImage()(img[0]))\n",
        "axs[0].set_title('original image')\n",
        "axs[1].imshow(ToPILImage()(img[1]))\n",
        "axs[1].set_title('2x downscale')\n",
        "axs[2].imshow(ToPILImage()(img[2]))\n",
        "axs[2].set_title('4x downscale')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N0ZmnHGJy2FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SRGAN Generator \n",
        "# adapted from https://github.com/Lornatang/SRGAN-PyTorch/blob/main/model.py\n",
        "\n",
        "def ResidualConvBlock(channels):\n",
        "    return nn.Sequential(nn.Conv2d(in_channels = channels, \n",
        "                                   out_channels = channels, \n",
        "                                   kernel_size = (3, 3), \n",
        "                                   stride = (1, 1), \n",
        "                                   padding = (1, 1), \n",
        "                                   bias=False),\n",
        "                         nn.BatchNorm2d(channels),\n",
        "                         nn.PReLU(), # great for mapping low-resolution images to high-resolution images\n",
        "                         nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "                         nn.BatchNorm2d(channels), \n",
        "                         nn.Dropout(p=0.05))\n",
        "\n",
        "# only zoom in by 2x each time\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        in_channels = 3\n",
        "        out_channels = 3\n",
        "        channels = 64 # this is the intermediate channels in the network\n",
        "\n",
        "        # low frequency information extraction layer\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, channels, (9, 9), (1, 1), (4, 4)),\n",
        "            nn.PReLU())\n",
        "\n",
        "        # 5 Residual Blocks (note that there will be an element wise sum)\n",
        "        self.rcb1 = ResidualConvBlock(channels)\n",
        "        self.rcb2 = ResidualConvBlock(channels)\n",
        "        self.rcb3 = ResidualConvBlock(channels)\n",
        "        self.rcb4 = ResidualConvBlock(channels)\n",
        "        self.rcb5 = ResidualConvBlock(channels)\n",
        "\n",
        "        # high-frequency information linear fusion layer\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels = channels, \n",
        "                                             out_channels = channels, \n",
        "                                             kernel_size = (3, 3), \n",
        "                                             stride = (1, 1), \n",
        "                                             padding = (1, 1), \n",
        "                                             bias=False),\n",
        "                                   nn.BatchNorm2d(channels))\n",
        "\n",
        "        # zoom block (we will only be zooming up by factor 2 each time)\n",
        "        self.ub1 = nn.Sequential(nn.Conv2d(in_channels = channels,\n",
        "                                          out_channels = channels * 4,\n",
        "                                          kernel_size = (3, 3), \n",
        "                                          stride = (1, 1), \n",
        "                                          padding = (1, 1)),\n",
        "                                nn.PixelShuffle(2),\n",
        "                                nn.PReLU(), \n",
        "                                nn.AvgPool2d(2)) # note the addition of the average pooling layer\n",
        "        self.ub2 = nn.Sequential(nn.Conv2d(in_channels = channels,\n",
        "                                          out_channels = channels * 4,\n",
        "                                          kernel_size = (3, 3), \n",
        "                                          stride = (1, 1), \n",
        "                                          padding = (1, 1)),\n",
        "                                nn.PixelShuffle(2),\n",
        "                                nn.PReLU(), \n",
        "                                nn.AvgPool2d(2)) # note the addition of the average pooling layer\n",
        "\n",
        "        # reconstruction block\n",
        "        self.conv3 = nn.Conv2d(in_channels = channels, \n",
        "                               out_channels = out_channels, \n",
        "                               kernel_size = (9, 9), \n",
        "                               stride = (1, 1), \n",
        "                               padding = (4, 4)) # retains the dimension of the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "        rcb1 = self.rcb1(out1)\n",
        "        rcb2 = self.rcb2(torch.add(out1, rcb1)) # included the skip connections\n",
        "        rcb3 = self.rcb3(torch.add(rcb1, rcb2))\n",
        "        rcb4 = self.rcb4(torch.add(rcb2, rcb3))\n",
        "        out2 = self.rcb5(torch.add(rcb3, rcb4))\n",
        "        out2 = self.conv2(out2)\n",
        "        out = torch.add(out1, out2)\n",
        "        out = self.ub1(out)\n",
        "        out = self.ub2(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "# unit test\n",
        "generator = Generator()\n",
        "sample = next(iter(dataloaders['train']))[:,2]\n",
        "output = generator(sample)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "EtBfuVfL1xb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DiscriminatorConvBlock(in_channels, out_channels): \n",
        "    return nn.Sequential(nn.Conv2d(in_channels = in_channels, \n",
        "                                   out_channels = in_channels, \n",
        "                                   kernel_size = (3, 3), \n",
        "                                   stride = (2, 2), \n",
        "                                   padding = (1, 1), \n",
        "                                   bias=False),\n",
        "                         nn.BatchNorm2d(in_channels),\n",
        "                         nn.LeakyReLU(0.2, True),\n",
        "                         nn.Conv2d(in_channels = in_channels, \n",
        "                                   out_channels = out_channels, \n",
        "                                   kernel_size = (3, 3), \n",
        "                                   stride = (2, 2), \n",
        "                                   padding = (1, 1), \n",
        "                                   bias=False),\n",
        "                         nn.BatchNorm2d(out_channels),\n",
        "                         nn.LeakyReLU(0.2, True), \n",
        "                         nn.Dropout(p=0.05))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # input shape (6) x 218 x 178\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, (3, 3), (1, 1), (1, 1), bias=True),\n",
        "                                   nn.LeakyReLU(0.2, True))\n",
        "        # input shape (64) x 218 x 178\n",
        "        self.conv2 = DiscriminatorConvBlock(64, 128)\n",
        "        # input shape (128) x 55, 45\n",
        "        self.conv3 = DiscriminatorConvBlock(128, 256)\n",
        "        # input shape (256) x 14, 12\n",
        "        self.conv4 = DiscriminatorConvBlock(256, 512)\n",
        "        # input shape (1024) x 4, 3\n",
        "        self.conv5 = DiscriminatorConvBlock(512, 1024) \n",
        "        # input shape (1024) * 1 * 1\n",
        "        self.classifier = nn.Sequential(nn.Linear(1024, 1024),\n",
        "                                        nn.LeakyReLU(0.2, True),\n",
        "                                        nn.Dropout(p=0.05),\n",
        "                                        nn.Linear(1024, 1), \n",
        "                                        nn.Sigmoid()) # for BCE loss\n",
        "\n",
        "    def forward(self, lr, sr):\n",
        "        x = torch.cat((lr, sr), 1) # need both the input and the output to distinguish\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)   \n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "# unit test\n",
        "discriminator = Discriminator()\n",
        "sample = next(iter(dataloaders['train']))\n",
        "\n",
        "output = discriminator(sample[:,1,:,:], sample[:,2,:,:])\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "BwLa4brJpKWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reference: https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee\n",
        "# content loss (loss based on the perceptual quality of the generated SR image as compared to the perceptual quality of the original HR image)\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ContentLoss, self).__init__()\n",
        "\n",
        "        # load the VGG19 model trained on the ImageNet dataset\n",
        "        # vgg: features (36 nodes) -> avg pool -> classifier\n",
        "\n",
        "        model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # standard basic (this is hardcoded to prevent modifications)\n",
        "        self.feature_model_extractor_node = \"features.35\" # extract the thirty-sixth layer output in the VGG19 model as the content loss\n",
        "        self.feature_model_normalize_mean = [0.485, 0.456, 0.406]\n",
        "        self.feature_model_normalize_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "        # normalize input\n",
        "        self.normalize = transforms.Normalize(self.feature_model_normalize_mean, self.feature_model_normalize_std)\n",
        "\n",
        "        # feature extractor\n",
        "        self.feature_extractor = create_feature_extractor(model, [self.feature_model_extractor_node])\n",
        "\n",
        "        # set to validation mode\n",
        "        self.feature_extractor.eval()\n",
        " \n",
        "        # Freeze model parameters.\n",
        "        for model_parameters in self.feature_extractor.parameters():\n",
        "            model_parameters.requires_grad = False\n",
        "\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, out_image, target_image):\n",
        "        # put feature extractor to the same device\n",
        "        if out_image.is_cuda: \n",
        "            self.feature_extractor.cuda()\n",
        "        # standardized operations\n",
        "        out_image = self.normalize(out_image)\n",
        "        target_image = self.normalize(target_image)\n",
        "        out_feature = self.feature_extractor(out_image)[self.feature_model_extractor_node]\n",
        "        target_feature = self.feature_extractor(target_image)[self.feature_model_extractor_node]\n",
        "        # find the feature map mse between the two images\n",
        "        loss = self.mse_loss(target_feature, out_feature)\n",
        "        return loss\n",
        "\n",
        "# reference: https://towardsdatascience.com/super-resolution-a-basic-study-e01af1449e13\n",
        "# total variation loss (supress the noise in the generated image)\n",
        "class TVLoss(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(TVLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = BATCH_SIZE\n",
        "        h_x = height\n",
        "        w_x = width\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
        "        return (h_tv + w_tv)/(w_x * h_x * 3 * batch_size)\n",
        "\n",
        "# generator loss\n",
        "# reference: https://github.com/leftthomas/SRGAN\n",
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        # Load the VGG19 model trained on the ImageNet dataset.\n",
        "        self.content_loss = ContentLoss()\n",
        "        self.tv_loss = TVLoss()\n",
        "        self.pixel_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, out_labels, out_images, target_images):\n",
        "        adversarial_loss = torch.mean(1 - out_labels)\n",
        "        content_loss = self.content_loss(out_images, target_images)\n",
        "        tv_loss = self.tv_loss(out_images)\n",
        "        pixel_loss = self.pixel_loss(out_images, target_images)\n",
        "        # print('adversarial_loss', adversarial_loss)\n",
        "        # print('content loss:', content_loss)\n",
        "        # print('total variation loss:', tv_loss)\n",
        "        # print('pixel_loss:', pixel_loss)\n",
        "        \n",
        "        return 0.01 * adversarial_loss + content_loss + 0.1 * tv_loss + pixel_loss\n",
        "\n",
        "# unit test\n",
        "sample = next(iter(dataloaders['train']))\n",
        "gl = GeneratorLoss()\n",
        "print('generator loss:', gl(torch.ones(BATCH_SIZE), sample[:,2,:,:], sample[:,0,:,:]))\n",
        "print('generator loss:', gl(torch.zeros(BATCH_SIZE), sample[:,2,:,:], sample[:,0,:,:]))\n",
        "print('generator loss:', gl(torch.zeros(BATCH_SIZE), sample[:,0,:,:], sample[:,0,:,:]))"
      ],
      "metadata": {
        "id": "bSOOc1sV2dyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "# create a place to save memory\n",
        "model_path = './model'\n",
        "if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path) # save all models to this path"
      ],
      "metadata": {
        "id": "vduZARbNZfWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train generator network first (warm start)\n",
        "\n",
        "# parameters for training the generator network (round 1)\n",
        "device = 'cuda:0'\n",
        "model_g = Generator().to(device)\n",
        "optimizer_g = optim.Adam(model_g.parameters(), lr=0.001) \n",
        "scheduler_g = lr_scheduler.StepLR(optimizer_g, step_size = 8, gamma = 0.5)\n",
        "criterion_g = GeneratorLoss()\n",
        "num_epochs = 5 # we just want to warm start the generator here"
      ],
      "metadata": {
        "id": "O3oj-fB7YBBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the generator Model\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "best_loss = 100.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # training step\n",
        "    model_g.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    train_running_loss = 0\n",
        "    for data in tqdm(dataloaders['train']):\n",
        "        lr = data[:,2,:,:].to(device) # 4x scale\n",
        "        sr = data[:,0,:,:].to(device) # 1x scale\n",
        "        optimizer_g.zero_grad()\n",
        "        outputs = model_g(lr)\n",
        "        loss = criterion_g(torch.ones(output.shape, device = device), outputs, sr) # adverserial loss is 0\n",
        "        loss.backward()\n",
        "        optimizer_g.step()        \n",
        "        train_running_loss += loss.item()\n",
        "    scheduler_g.step()\n",
        "    train_loss = train_running_loss * BATCH_SIZE/dataset_sizes['train']\n",
        "    train_loss_list.append(train_loss)\n",
        "\n",
        "    # validation step\n",
        "    model_g.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    # visualize how the mask prediction changes over time\n",
        "\n",
        "    val_running_loss = 0\n",
        "    for data in dataloaders['test']:\n",
        "        lr = data[:,2,:,:].to(device) # 4x scale\n",
        "        sr = data[:,0,:,:].to(device) # 1x scale\n",
        "        outputs = model_g(lr)\n",
        "        loss = criterion_g(torch.ones(output.shape, device = device), outputs, sr) # adverserial loss is 0\n",
        "        val_running_loss += loss.item()\n",
        "    val_loss = val_running_loss * BATCH_SIZE/dataset_sizes['test'] \n",
        "    val_loss_list.append(val_loss)\n",
        "\n",
        "    # update the best model\n",
        "    if val_loss < best_loss: \n",
        "        best_loss = val_loss\n",
        "        torch.save(model_g.state_dict(), './model/generator')\n",
        "\n",
        "    print(f'epoch: {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.8f}, Test Loss: {val_loss:.8f}')"
      ],
      "metadata": {
        "id": "XINWQb6mcNiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unit test\n",
        "# sample output from pretrained generator\n",
        "model_g.to('cpu')\n",
        "img = next(iter(dataloaders['test']))\n",
        "lr = img[0][2]\n",
        "gen_sr = model_g(img[:,2,:,:])[0]\n",
        "sr = img[0][0]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "# Display the LR and HR images using matplotlib\n",
        "axs[0].imshow(ToPILImage()(sr))\n",
        "axs[0].set_title('sr image 1x')\n",
        "axs[1].imshow(ToPILImage()(gen_sr))\n",
        "axs[1].set_title('gen sr image 1x')\n",
        "axs[2].imshow(ToPILImage()(lr))\n",
        "axs[2].set_title('lr image 4x')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BkygF-G1nxmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters for training the discriminator network\n",
        "model_g.load_state_dict(torch.load('./model/generator'))\n",
        "model_g.to(device)\n",
        "model_d = Discriminator().to(device)\n",
        "optimizer_d = optim.Adam(model_d.parameters(), lr=0.01)\n",
        "criterion_d = nn.BCELoss()\n",
        "num_epochs = 2 # warm start so does not need that many"
      ],
      "metadata": {
        "id": "hE6kEsG5HB31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# warm start the discriminator model (4x -> 1x)\n",
        "# note that we are simplifying the loss function for the discriminator\n",
        "discriminator_loss_list = []\n",
        "discriminator_val_loss_list = []\n",
        "best_loss = 100.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train step\n",
        "    running_loss_d = 0\n",
        "    running_loss_g = 0\n",
        "    model_d.train()\n",
        "    model_g.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    for data in tqdm(dataloaders['train']):\n",
        "        lr = data[:,2,:,:].to(device) # 4x scale\n",
        "        sr = data[:,0,:,:].to(device) # 1x scale\n",
        "        # log(D(x))\n",
        "        optimizer_d.zero_grad()\n",
        "        output = model_d(lr, sr)\n",
        "        loss_d_real = criterion_d(output, torch.ones(output.shape, device = device))\n",
        "        loss_d_real.backward()\n",
        "        # log(1 - D(G(z)))\n",
        "        fake_sr = model_g(lr)\n",
        "        output = model_d(lr, fake_sr)\n",
        "        loss_d_fake = criterion_d(output, torch.zeros(output.shape, device = device))\n",
        "        loss_d_fake.backward(retain_graph=True)\n",
        "        loss_d = loss_d_real + loss_d_fake\n",
        "        optimizer_d.step()\n",
        "        running_loss_d += loss_d.item()\n",
        "    discriminator_loss = running_loss_d/dataset_sizes['train']\n",
        "    discriminator_loss_list.append(discriminator_loss)\n",
        "\n",
        "    # validation step\n",
        "    model_d.eval()\n",
        "    val_running_loss_d = 0\n",
        "    torch.set_grad_enabled(False)\n",
        "    for data in dataloaders['test']:\n",
        "        lr = data[:,2,:,:].to(device) # 4x scale\n",
        "        sr = data[:,0,:,:].to(device) # 1x scale\n",
        "        # log(D(x))\n",
        "        output = model_d(lr, sr)\n",
        "        loss_d_real = criterion_d(output, torch.ones(output.shape, device = device))\n",
        "        # log(1 - D(G(z)))\n",
        "        fake_sr = model_g(lr)\n",
        "        output = model_d(lr, fake_sr)\n",
        "        loss_d_fake = criterion_d(output, torch.zeros(output.shape, device = device))\n",
        "        loss_d = loss_d_real + loss_d_fake\n",
        "        val_running_loss_d += loss_d.item()\n",
        "    val_loss = val_running_loss_d/dataset_sizes['test']\n",
        "    discriminator_val_loss_list.append(val_loss)\n",
        "    # update the best model\n",
        "    if val_loss < best_loss: \n",
        "        # save the weights\n",
        "        best_loss = val_loss\n",
        "        torch.save(model_d.state_dict(), './model/discriminator')\n",
        "\n",
        "    # print the progress to determine if the progress has stagnated\n",
        "    print(f'epoch: {epoch+1}/{num_epochs}, Discriminator Loss: {discriminator_loss:.4f}, Discriminator Validation Loss: {val_loss:.8f}')"
      ],
      "metadata": {
        "id": "OdMWJNe0GLnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrained generator and discriminator\n",
        "model_g.load_state_dict(torch.load('./model/generator'))\n",
        "model_g.to(device)\n",
        "model_d.load_state_dict(torch.load('./model/discriminator'))\n",
        "model_d.to(device)\n",
        "\n",
        "# set the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# train the SRGAN model (4x -> 1x)\n",
        "discriminator_loss_list = []\n",
        "generator_loss_list = []\n",
        "generator_val_loss_list = []\n",
        "best_loss = 100.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train step\n",
        "    running_loss_d = 0\n",
        "    running_loss_g = 0\n",
        "    model_d.train()\n",
        "    model_g.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    for data in tqdm(dataloaders['train']):\n",
        "        lr = data[:,2,:,:].to(device) # 4x scale\n",
        "        sr = data[:,0,:,:].to(device) # 1x scale\n",
        "        ############################\n",
        "        # (1) update D network: maximize log(D(x))+log(1-D(G(z)))\n",
        "        ###########################\n",
        "        optimizer_d.zero_grad()\n",
        "        output = model_d(lr, sr)\n",
        "        loss_d_real = criterion_d(output, torch.ones(output.shape, device = device))\n",
        "        loss_d_real.backward()\n",
        "        # log(1 - D(G(z)))\n",
        "        fake_sr = model_g(lr)\n",
        "        output = model_d(lr, fake_sr)\n",
        "        loss_d_fake = criterion_d(output, torch.zeros(output.shape, device = device))\n",
        "        loss_d_fake.backward(retain_graph=True)\n",
        "        loss_d = loss_d_real + loss_d_fake\n",
        "        optimizer_d.step()\n",
        "        running_loss_d += loss_d.item()\n",
        "        ############################\n",
        "        # (2) update G network: minimize 1-D(G(z)) + Content Loss + TV Loss\n",
        "        ###########################\n",
        "        optimizer_g.zero_grad()\n",
        "        # note that fake labels are real for generator cost\n",
        "        loss_g = criterion_g(out_labels = output, out_images = fake_sr, target_images = sr) \n",
        "        loss_g.backward()\n",
        "        optimizer_g.step()\n",
        "        running_loss_g += loss_g.item()\n",
        "    discriminator_loss = running_loss_d/dataset_sizes['train']\n",
        "    discriminator_loss_list.append(discriminator_loss)\n",
        "    generator_loss = running_loss_g/dataset_sizes['train']\n",
        "    generator_loss_list.append(generator_loss)\n",
        "\n",
        "    # validation step\n",
        "    model_g.eval()\n",
        "    val_running_loss = 0\n",
        "    torch.set_grad_enabled(False)\n",
        "    for data in dataloaders['test']:\n",
        "        lr = data[:,2,:,:].to(device) # 4x scale\n",
        "        sr = data[:,0,:,:].to(device) # 1x scale\n",
        "        fake_sr = model_g(lr)\n",
        "        output = model_d(lr, fake_sr) \n",
        "        loss = criterion_g(out_labels = output, out_images = fake_sr, target_images = sr) \n",
        "        val_running_loss += loss.item() \n",
        "    val_loss = val_running_loss/dataset_sizes['test']\n",
        "    generator_val_loss_list.append(val_loss)\n",
        "    # update the best model\n",
        "    if val_loss < best_loss: \n",
        "        # save the weights\n",
        "        best_loss = val_loss\n",
        "        torch.save(model_g.state_dict(), './model/gan_generator')\n",
        "        torch.save(model_d.state_dict(), './model/gan_discriminator')\n",
        "\n",
        "    # print the progress to determine if the progress has stagnated\n",
        "    print(f'epoch: {epoch+1}/{num_epochs}, Generator Loss: {generator_loss:.4f}, Discriminator Loss: {discriminator_loss:.4f}, Generator Validation Loss: {val_loss:.8f}')"
      ],
      "metadata": {
        "id": "3p3LUq2kMgew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unit test\n",
        "# sample output from pretrained generator\n",
        "model_g.to('cpu')\n",
        "img = next(iter(dataloaders['test']))\n",
        "lr = img[0][2]\n",
        "gen_sr = model_g(img[:,2,:,:])[0]\n",
        "sr = img[0][0]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "# Display the LR and HR images using matplotlib\n",
        "axs[0].imshow(ToPILImage()(sr))\n",
        "axs[0].set_title('sr image 1x')\n",
        "axs[1].imshow(ToPILImage()(gen_sr))\n",
        "axs[1].set_title('gen sr image 1x')\n",
        "axs[2].imshow(ToPILImage()(lr))\n",
        "axs[2].set_title('lr image 4x')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-KXmsb5-1-Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adapted from xingyue model\n",
        "\n",
        "# !pip install torchmetrics\n",
        "# !pip install lpips\n",
        "from torchmetrics import PeakSignalNoiseRatio\n",
        "from torchmetrics import StructuralSimilarityIndexMeasure\n",
        "import lpips\n",
        "psnr = PeakSignalNoiseRatio().to(device)\n",
        "ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
        "loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "# Evaluate model performance on test dataset (modified from xinyue model)\n",
        "l1_loss = []\n",
        "mse_loss = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "output_list = []\n",
        "lpips_list = []\n",
        "criterion1 = nn.L1Loss()\n",
        "criterion2 = nn.MSELoss()\n",
        "model_g.to(device)\n",
        "model_g.eval()\n",
        "with torch.no_grad():\n",
        "    for data in dataloaders['test']:\n",
        "        lr = data[:,2,:,:].to(device) # 4x scale\n",
        "        sr = data[:,0,:,:].to(device) # 1x scale\n",
        "        fake_sr = model_g(lr)\n",
        "        # Compute L1 loss\n",
        "        loss = criterion1(fake_sr, sr)\n",
        "        l1_loss.append(loss.item())\n",
        "\n",
        "        # Compute MSE loss\n",
        "        loss2 = criterion2(fake_sr, sr)\n",
        "        mse_loss.append(loss2.item())\n",
        "\n",
        "        # Compute PSNR \n",
        "        psnr_value = psnr(fake_sr, sr)\n",
        "        psnr_value = psnr_value.clone().cpu().detach().numpy()\n",
        "        psnr_list.append(psnr_value)\n",
        "        \n",
        "        # Compute SSIM\n",
        "        ssim_value = ssim(fake_sr, sr)\n",
        "        ssim_value = ssim_value.clone().cpu().detach().numpy()\n",
        "        ssim_list.append(ssim_value)\n",
        "\n",
        "        # Compute LPIPS\n",
        "        d = loss_fn.forward(fake_sr, sr).clone().cpu().detach().numpy()\n",
        "        lpips_list.append(d)\n",
        "        \n",
        "print('l1_loss:', np.mean(l1_loss))\n",
        "print('mse_loss', np.mean(mse_loss))\n",
        "print('psnr:', np.mean(psnr_list))\n",
        "print('lpips:', np.mean(lpips_list))\n",
        "print('ssim:', np.mean(ssim_list))"
      ],
      "metadata": {
        "id": "yKjDHm5D2QYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UnW_2_Tyjqma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}