{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import itertools\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torchvision.transforms import ToTensor, Normalize, ToPILImage\n",
        "from torchvision.transforms.functional import hflip, vflip, rotate, adjust_hue\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from torchsummary import summary\n",
        "\n",
        "# pip install torchmetrics\n",
        "import torchmetrics\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "_ybY5acYmvDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key Parameters \n",
        "download_original = 0 # download the slide image and masks from google bucket\n",
        "generate_data = 1 # generate the downscale data\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# original width and height of image (standardized in all images)\n",
        "width, height = 600,600\n",
        "lr_width, lr_height = 150, 150 # 4x reduction"
      ],
      "metadata": {
        "id": "CQtY57o7m-nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the image the google bucket that I set up \n",
        "# credit: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
        "if download_original == 1: \n",
        "    image_url = 'https://storage.googleapis.com/acv_project/landscape_img.zip'\n",
        "    !curl -O $image_url\n",
        "    !unzip landscape_img\n",
        "    !rm landscape_img.zip\n",
        "    # push all the images into a single place:\n",
        "    os.mkdir('./original') \n",
        "    for place in os.listdir('./landscape_img'): \n",
        "        for img in os.listdir(os.path.join('./landscape_img', place)):\n",
        "            shutil.copy(os.path.join(os.path.join('./landscape_img', place), img), './original')"
      ],
      "metadata": {
        "id": "F_k_0FDlsDbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if generate_data == 1: \n",
        "    # train test split\n",
        "    # split by id \n",
        "    img_id = os.listdir('./original')\n",
        "    train_id, test_id = train_test_split(img_id, test_size=0.1, random_state=1)      \n",
        "\n",
        "    # create test and train folder    \n",
        "    !rm -rf ./train\n",
        "    os.mkdir('./train') \n",
        "    os.mkdir('./train/original') # save all train data to this path\n",
        "\n",
        "    !rm -rf ./test\n",
        "    os.mkdir('./test') \n",
        "    os.mkdir('./test/original') # save all train data to this path\n",
        "\n",
        "    # save img into the respective folders as 1x downsampling\n",
        "    for id in train_id: \n",
        "        shutil.copy('./original/' + id, './train/original')\n",
        "\n",
        "    for id in test_id: \n",
        "        shutil.copy('./original/' + id, './test/original')\n",
        "\n",
        "    # downscale images via Pillow \n",
        "    # this will take quite some time to run\n",
        "    downscale_factor_list = [1, 2, 4]\n",
        "    for path in ['./train/', './test/']:\n",
        "        img_id_list = os.listdir(path + 'original/')\n",
        "        for img_id in tqdm(img_id_list): \n",
        "            img = Image.open(path + 'original/' + img_id)\n",
        "            img_arr_1x = np.array(img) # 1x downscale\n",
        "            # downscale and upscale again (bicubic method)\n",
        "            img_4x = img.resize((lr_width, lr_height)) \n",
        "            img_4x = img_4x.resize((width, height))\n",
        "            img_arr_4x = np.array(img_4x) # 4x downscale\n",
        "            np.save(os.path.join(path, img_id[:-4]), np.stack((img_arr_1x, img_arr_4x)))\n",
        "    \n",
        "    !rm -rf ./train/original\n",
        "    !rm -rf ./test/original"
      ],
      "metadata": {
        "id": "4tX5pZ87vUBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset and dataloader\n",
        "class SatelliteDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_files = os.listdir(img_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # load the image from disk \n",
        "        img_arr = np.load(os.path.join(self.img_dir, self.img_files[idx]))\n",
        "        img_1x = Image.fromarray(img_arr[0])\n",
        "        img_4x = Image.fromarray(img_arr[1])\n",
        "        img_4x = img_4x.resize((lr_width, lr_height)) \n",
        "\n",
        "        # apply flip, rotate, and convert to tensor \n",
        "        # flipping\n",
        "  \n",
        "        c = np.random.randint(0,3)\n",
        "        if c == 1: \n",
        "            img_1x = hflip(img_1x)\n",
        "            img_4x = hflip(img_4x)\n",
        "        elif c == 2: \n",
        "            img_1x = vflip(img_1x)\n",
        "            img_4x = vflip(img_4x)\n",
        "        elif c == 3: \n",
        "            img_1x = vflip(img_1x)\n",
        "            img_4x = vflip(img_4x)\n",
        "            mask = vflip(mask)\n",
        "\n",
        "        # rotation\n",
        "        c = np.random.randint(0,3)\n",
        "        img_1x  = rotate(img_1x, 90*c)\n",
        "        img_4x  = rotate(img_4x, 90*c)\n",
        "\n",
        "        # to tensor \n",
        "        img_1x = ToTensor()(img_1x)\n",
        "        img_4x = ToTensor()(img_4x)\n",
        "\n",
        "        return img_4x, img_1x\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Filter failed images first\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "    \n",
        "    # Now collate into mini-batches\n",
        "    lr = torch.stack([b[0] for b in batch]) \n",
        "    sr = torch.stack([b[1] for b in batch]) \n",
        "    return lr, sr\n",
        "\n",
        "# implement custom image_dataset and wrap it with the dataloader\n",
        "image_datasets = {x: SatelliteDataset(os.path.join('./', x)) for x in ['train', 'test']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, \n",
        "                                              shuffle=True, num_workers=0, collate_fn = collate_fn)\n",
        "              for x in ['train', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "print('size of dataset', dataset_sizes)"
      ],
      "metadata": {
        "id": "Pqd_N35owQBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(dataloaders['train']))[0]\n",
        "print(sample.shape)"
      ],
      "metadata": {
        "id": "wHCMB-Yt8y7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample output\n",
        "sample = next(iter(dataloaders['train']))\n",
        "lr = sample[0][0]\n",
        "sr = sample[1][0]\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
        "\n",
        "# Display the LR and HR images using matplotlib\n",
        "axs[0].imshow(ToPILImage()(sr))\n",
        "axs[0].set_title('original image')\n",
        "axs[1].imshow(ToPILImage()(lr))\n",
        "axs[1].set_title('4x downscale')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Od5Gg-azzdjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SRGAN Generator \n",
        "# adapted from https://github.com/Lornatang/SRGAN-PyTorch/blob/main/model.py\n",
        "\n",
        "def ResidualConvBlock(channels):\n",
        "    return nn.Sequential(nn.Conv2d(in_channels = channels, \n",
        "                                   out_channels = channels, \n",
        "                                   kernel_size = (3, 3), \n",
        "                                   stride = (1, 1), \n",
        "                                   padding = (1, 1), \n",
        "                                   bias=False),\n",
        "                         nn.BatchNorm2d(channels),\n",
        "                         nn.PReLU(), # great for mapping low-resolution images to high-resolution images\n",
        "                         nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "                         nn.BatchNorm2d(channels), \n",
        "                         nn.Dropout(p=0.05))\n",
        "\n",
        "# only zoom in by 2x each time\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        in_channels = 3\n",
        "        out_channels = 3\n",
        "        channels = 64 # this is the intermediate channels in the network\n",
        "\n",
        "        # low frequency information extraction layer\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, channels, (9, 9), (1, 1), (4, 4)),\n",
        "            nn.PReLU())\n",
        "\n",
        "        # 5 Residual Blocks (note that there will be an element wise sum)\n",
        "        self.rcb1 = ResidualConvBlock(channels)\n",
        "        self.rcb2 = ResidualConvBlock(channels)\n",
        "        self.rcb3 = ResidualConvBlock(channels)\n",
        "        self.rcb4 = ResidualConvBlock(channels)\n",
        "        self.rcb5 = ResidualConvBlock(channels)\n",
        "\n",
        "        # high-frequency information linear fusion layer\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels = channels, \n",
        "                                             out_channels = channels, \n",
        "                                             kernel_size = (3, 3), \n",
        "                                             stride = (1, 1), \n",
        "                                             padding = (1, 1), \n",
        "                                             bias=False),\n",
        "                                   nn.BatchNorm2d(channels))\n",
        "\n",
        "        # zoom block (we will only be zooming up by factor 2 each time)\n",
        "        self.ub1 = nn.Sequential(nn.Conv2d(in_channels = channels,\n",
        "                                          out_channels = channels * 4,\n",
        "                                          kernel_size = (3, 3), \n",
        "                                          stride = (1, 1), \n",
        "                                          padding = (1, 1)),\n",
        "                                nn.PixelShuffle(2),\n",
        "                                nn.PReLU()) \n",
        "        self.ub2 = nn.Sequential(nn.Conv2d(in_channels = channels,\n",
        "                                          out_channels = channels * 4,\n",
        "                                          kernel_size = (3, 3), \n",
        "                                          stride = (1, 1), \n",
        "                                          padding = (1, 1)),\n",
        "                                nn.PixelShuffle(2),\n",
        "                                nn.PReLU()) \n",
        "        # reconstruction block\n",
        "        self.conv3 = nn.Conv2d(in_channels = channels, \n",
        "                               out_channels = out_channels, \n",
        "                               kernel_size = (9, 9), \n",
        "                               stride = (1, 1), \n",
        "                               padding = (4, 4)) # retains the dimension of the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "        rcb1 = self.rcb1(out1)\n",
        "        rcb2 = self.rcb2(torch.add(out1, rcb1)) # included the skip connections\n",
        "        rcb3 = self.rcb3(torch.add(rcb1, rcb2))\n",
        "        rcb4 = self.rcb4(torch.add(rcb2, rcb3))\n",
        "        out2 = self.rcb5(torch.add(rcb3, rcb4))\n",
        "        out2 = self.conv2(out2)\n",
        "        out = torch.add(out1, out2)\n",
        "        out = self.ub1(out)\n",
        "        out = self.ub2(out)\n",
        "        out = self.conv3(out)\n",
        "        return out\n",
        "\n",
        "# unit test\n",
        "generator = Generator()\n",
        "sample = next(iter(dataloaders['train']))\n",
        "lr = sample[0]\n",
        "sr = sample[1]\n",
        "output = generator(lr)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "SMAnQvKqzfGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DiscriminatorConvBlock(in_channels, out_channels): \n",
        "    return nn.Sequential(nn.Conv2d(in_channels = in_channels, \n",
        "                                   out_channels = in_channels, \n",
        "                                   kernel_size = (3, 3), \n",
        "                                   stride = (2, 2), \n",
        "                                   padding = (1, 1), \n",
        "                                   bias=False),\n",
        "                         nn.BatchNorm2d(in_channels),\n",
        "                         nn.LeakyReLU(0.2, True),\n",
        "                         nn.Conv2d(in_channels = in_channels, \n",
        "                                   out_channels = out_channels, \n",
        "                                   kernel_size = (3, 3), \n",
        "                                   stride = (2, 2), \n",
        "                                   padding = (1, 1), \n",
        "                                   bias=False),\n",
        "                         nn.BatchNorm2d(out_channels),\n",
        "                         nn.LeakyReLU(0.2, True), \n",
        "                         nn.Dropout(p=0.05))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # input shape (3) x 600 x 600\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(3, 64, (3, 3), (1, 1), (1, 1), bias=True),\n",
        "                                   nn.LeakyReLU(0.2, True))\n",
        "        # input shape (64) x 150 x 150\n",
        "        self.conv2 = DiscriminatorConvBlock(64, 128)\n",
        "        # input shape (131) x 38, 38\n",
        "        self.conv3 = DiscriminatorConvBlock(128, 256)\n",
        "        self.conv4 = DiscriminatorConvBlock(256, 512)\n",
        "        self.conv5 = DiscriminatorConvBlock(512, 1024) \n",
        "        self.conv6 = DiscriminatorConvBlock(1024, 2048) \n",
        "        # input shape (2048) * 1, 1\n",
        "        self.classifier = nn.Sequential(nn.Linear(2048, 1024),\n",
        "                                        nn.LeakyReLU(0.2, True),\n",
        "                                        nn.Dropout(p=0.05),\n",
        "                                        nn.Linear(1024, 1), \n",
        "                                        nn.Sigmoid()) # for BCE loss\n",
        "\n",
        "    def forward(self, lr, sr):\n",
        "        out = self.conv1(sr)\n",
        "        out = self.conv2(out)\n",
        "        x = torch.cat((lr, out), 1) # need both the input and the output to distinguish\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)  \n",
        "        out = self.conv5(out)\n",
        "        out = self.conv6(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "# unit test\n",
        "discriminator = Discriminator()\n",
        "generator = Generator()\n",
        "sample = next(iter(dataloaders['train']))\n",
        "lr = sample[0]\n",
        "sr = sample[1]\n",
        "output = discriminator(lr, sr)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "7LcG0BIxzik7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reference: https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee\n",
        "# content loss (loss based on the perceptual quality of the generated SR image as compared to the perceptual quality of the original HR image)\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ContentLoss, self).__init__()\n",
        "\n",
        "        # load the VGG19 model trained on the ImageNet dataset\n",
        "        # vgg: features (36 nodes) -> avg pool -> classifier\n",
        "\n",
        "        model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # standard basic (this is hardcoded to prevent modifications)\n",
        "        self.feature_model_extractor_node = \"features.35\" # extract the thirty-sixth layer output in the VGG19 model as the content loss\n",
        "        self.feature_model_normalize_mean = [0.485, 0.456, 0.406]\n",
        "        self.feature_model_normalize_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "        # normalize input\n",
        "        self.normalize = transforms.Normalize(self.feature_model_normalize_mean, self.feature_model_normalize_std)\n",
        "\n",
        "        # feature extractor\n",
        "        self.feature_extractor = create_feature_extractor(model, [self.feature_model_extractor_node])\n",
        "\n",
        "        # set to validation mode\n",
        "        self.feature_extractor.eval()\n",
        " \n",
        "        # Freeze model parameters.\n",
        "        for model_parameters in self.feature_extractor.parameters():\n",
        "            model_parameters.requires_grad = False\n",
        "\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, out_image, target_image):\n",
        "        # put feature extractor to the same device\n",
        "        if out_image.is_cuda: \n",
        "            self.feature_extractor.cuda()\n",
        "        # standardized operations\n",
        "        out_image = self.normalize(out_image)\n",
        "        target_image = self.normalize(target_image)\n",
        "        out_feature = self.feature_extractor(out_image)[self.feature_model_extractor_node]\n",
        "        target_feature = self.feature_extractor(target_image)[self.feature_model_extractor_node]\n",
        "        # find the feature map mse between the two images\n",
        "        loss = self.mse_loss(target_feature, out_feature)\n",
        "        return loss\n",
        "\n",
        "# reference: https://towardsdatascience.com/super-resolution-a-basic-study-e01af1449e13\n",
        "# total variation loss (supress the noise in the generated image)\n",
        "class TVLoss(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(TVLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = BATCH_SIZE\n",
        "        h_x = height\n",
        "        w_x = width\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
        "        return (h_tv + w_tv)/(w_x * h_x * 3 * batch_size)\n",
        "\n",
        "# generator loss\n",
        "# reference: https://github.com/leftthomas/SRGAN\n",
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        # Load the VGG19 model trained on the ImageNet dataset.\n",
        "        self.max_pool = nn.MaxPool2d(4)\n",
        "        self.content_loss = ContentLoss()\n",
        "        self.tv_loss = TVLoss()\n",
        "        self.pixel_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, out_labels, out_images, target_images):\n",
        "        adversarial_loss = torch.mean(1 - out_labels)\n",
        "        content_loss = self.content_loss(self.max_pool(out_images), self.max_pool(target_images))\n",
        "        tv_loss = self.tv_loss(out_images)\n",
        "        pixel_loss = self.pixel_loss(out_images, target_images)\n",
        "        # print('adversarial_loss', adversarial_loss)\n",
        "        # print('content loss:', content_loss)\n",
        "        # print('total variation loss:', tv_loss)\n",
        "        # print('pixel_loss:', pixel_loss)\n",
        "        \n",
        "        return 0.01 * adversarial_loss + content_loss + 0.1 * tv_loss + pixel_loss\n",
        "\n",
        "# unit test\n",
        "sample = next(iter(dataloaders['train']))\n",
        "sr1 = sample[1]\n",
        "sample = next(iter(dataloaders['train']))\n",
        "sr2 = sample[1]\n",
        "gl = GeneratorLoss()\n",
        "print('generator loss:', gl(torch.ones(BATCH_SIZE), sr1, sr2))\n"
      ],
      "metadata": {
        "id": "RmEozk_a0qgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "# create a place to save memory\n",
        "model_path = './model'\n",
        "if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path) # save all models to this path"
      ],
      "metadata": {
        "id": "AFwnaJaID3Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train generator network first (warm start)\n",
        "\n",
        "# parameters for training the generator network (round 1)\n",
        "device = 'cuda:0'\n",
        "model_g = Generator().to(device)\n",
        "optimizer_g = optim.Adam(model_g.parameters(), lr=0.001) \n",
        "scheduler_g = lr_scheduler.StepLR(optimizer_g, step_size = 8, gamma = 0.5)\n",
        "criterion_g = GeneratorLoss()\n",
        "num_epochs = 5 # we just want to warm start the generator here"
      ],
      "metadata": {
        "id": "oFyWCMMGEa5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the generator Model\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "best_loss = 100.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # training step\n",
        "    model_g.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    train_running_loss = 0\n",
        "    for lr, sr in tqdm(dataloaders['train']):\n",
        "        lr = lr.to(device)\n",
        "        sr = sr.to(device)\n",
        "        optimizer_g.zero_grad()\n",
        "        outputs = model_g(lr)\n",
        "        loss = criterion_g(torch.ones(output.shape, device = device), outputs, sr) # adverserial loss is 0\n",
        "        loss.backward()\n",
        "        optimizer_g.step()        \n",
        "        train_running_loss += loss.item()\n",
        "    scheduler_g.step()\n",
        "    train_loss = train_running_loss * BATCH_SIZE/dataset_sizes['train']\n",
        "    train_loss_list.append(train_loss)\n",
        "\n",
        "    # validation step\n",
        "    model_g.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    # visualize how the mask prediction changes over time\n",
        "\n",
        "    val_running_loss = 0\n",
        "    for lr, sr in dataloaders['test']:\n",
        "        lr = lr.to(device)\n",
        "        sr = sr.to(device)\n",
        "        outputs = model_g(lr)\n",
        "        loss = criterion_g(torch.ones(output.shape, device = device), outputs, sr) # adverserial loss is 0\n",
        "        val_running_loss += loss.item()\n",
        "    val_loss = val_running_loss * BATCH_SIZE/dataset_sizes['test'] \n",
        "    val_loss_list.append(val_loss)\n",
        "\n",
        "    # update the best model\n",
        "    if val_loss < best_loss: \n",
        "        best_loss = val_loss\n",
        "        torch.save(model_g.state_dict(), './model/generator')\n",
        "\n",
        "    print(f'epoch: {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.8f}, Test Loss: {val_loss:.8f}')"
      ],
      "metadata": {
        "id": "b2Kbw0e9DhyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unit test\n",
        "# sample output from pretrained generator\n",
        "model_g.to('cpu')\n",
        "img = next(iter(dataloaders['test']))\n",
        "lr = img[0]\n",
        "gen_sr = model_g(lr)[0]\n",
        "lr = lr[0]\n",
        "sr = img[1][0]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "# Display the LR and HR images using matplotlib\n",
        "axs[0].imshow(ToPILImage()(sr))\n",
        "axs[0].set_title('sr image 1x')\n",
        "axs[1].imshow(ToPILImage()(gen_sr))\n",
        "axs[1].set_title('gen image 1x')\n",
        "axs[2].imshow(ToPILImage()(lr))\n",
        "axs[2].set_title('lr image 4x')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2yNTcu3xINvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters for training the discriminator network\n",
        "model_g.load_state_dict(torch.load('./model/generator'))\n",
        "model_g.to(device)\n",
        "model_d = Discriminator().to(device)\n",
        "optimizer_d = optim.Adam(model_d.parameters(), lr=0.01)\n",
        "criterion_d = nn.BCELoss()\n",
        "num_epochs = 2 # warm start so does not need that many"
      ],
      "metadata": {
        "id": "Y8SLGEPIEcVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# warm start the discriminator model (4x -> 1x)\n",
        "# note that we are simplifying the loss function for the discriminator\n",
        "discriminator_loss_list = []\n",
        "discriminator_val_loss_list = []\n",
        "best_loss = 100.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train step\n",
        "    running_loss_d = 0\n",
        "    running_loss_g = 0\n",
        "    model_d.train()\n",
        "    model_g.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    for lr, sr in tqdm(dataloaders['train']):\n",
        "        lr = lr.to(device)\n",
        "        sr = sr.to(device)\n",
        "        # log(D(x))\n",
        "        optimizer_d.zero_grad()\n",
        "        output = model_d(lr, sr)\n",
        "        loss_d_real = criterion_d(output, torch.ones(output.shape, device = device))\n",
        "        loss_d_real.backward()\n",
        "        # log(1 - D(G(z)))\n",
        "        fake_sr = model_g(lr)\n",
        "        output = model_d(lr, fake_sr)\n",
        "        loss_d_fake = criterion_d(output, torch.zeros(output.shape, device = device))\n",
        "        loss_d_fake.backward(retain_graph=True)\n",
        "        loss_d = loss_d_real + loss_d_fake\n",
        "        optimizer_d.step()\n",
        "        running_loss_d += loss_d.item()\n",
        "    discriminator_loss = running_loss_d/dataset_sizes['train']\n",
        "    discriminator_loss_list.append(discriminator_loss)\n",
        "\n",
        "    # validation step\n",
        "    model_d.eval()\n",
        "    val_running_loss_d = 0\n",
        "    torch.set_grad_enabled(False)\n",
        "    for lr, sr in dataloaders['test']:\n",
        "        lr = lr.to(device)\n",
        "        sr = sr.to(device)\n",
        "        # log(D(x))\n",
        "        output = model_d(lr, sr)\n",
        "        loss_d_real = criterion_d(output, torch.ones(output.shape, device = device))\n",
        "        # log(1 - D(G(z)))\n",
        "        fake_sr = model_g(lr)\n",
        "        output = model_d(lr, fake_sr)\n",
        "        loss_d_fake = criterion_d(output, torch.zeros(output.shape, device = device))\n",
        "        loss_d = loss_d_real + loss_d_fake\n",
        "        val_running_loss_d += loss_d.item()\n",
        "    val_loss = val_running_loss_d/dataset_sizes['test']\n",
        "    discriminator_val_loss_list.append(val_loss)\n",
        "    # update the best model\n",
        "    if val_loss < best_loss: \n",
        "        # save the weights\n",
        "        best_loss = val_loss\n",
        "        torch.save(model_d.state_dict(), './model/discriminator')"
      ],
      "metadata": {
        "id": "MLL7ix0MGWNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrained generator and discriminator\n",
        "model_g.load_state_dict(torch.load('./model/generator'))\n",
        "model_g.to(device)\n",
        "model_d.load_state_dict(torch.load('./model/discriminator'))\n",
        "model_d.to(device)\n",
        "\n",
        "# set the number of epochs\n",
        "num_epochs = 15\n",
        "\n",
        "# train the SRGAN model (4x -> 1x)\n",
        "discriminator_loss_list = []\n",
        "generator_loss_list = []\n",
        "generator_val_loss_list = []\n",
        "best_loss = 100.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train step\n",
        "    running_loss_d = 0\n",
        "    running_loss_g = 0\n",
        "    model_d.train()\n",
        "    model_g.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    for lr, sr in tqdm(dataloaders['train']):\n",
        "        lr = lr.to(device)\n",
        "        sr = sr.to(device)\n",
        "        ############################\n",
        "        # (1) update D network: maximize log(D(x))+log(1-D(G(z)))\n",
        "        ###########################\n",
        "        optimizer_d.zero_grad()\n",
        "        output = model_d(lr, sr)\n",
        "        loss_d_real = criterion_d(output, torch.ones(output.shape, device = device))\n",
        "        loss_d_real.backward()\n",
        "        # log(1 - D(G(z)))\n",
        "        fake_sr = model_g(lr)\n",
        "        output = model_d(lr, fake_sr)\n",
        "        loss_d_fake = criterion_d(output, torch.zeros(output.shape, device = device))\n",
        "        loss_d_fake.backward(retain_graph=True)\n",
        "        loss_d = loss_d_real + loss_d_fake\n",
        "        optimizer_d.step()\n",
        "        running_loss_d += loss_d.item()\n",
        "        ############################\n",
        "        # (2) update G network: minimize 1-D(G(z)) + Content Loss + TV Loss\n",
        "        ###########################\n",
        "        optimizer_g.zero_grad()\n",
        "        # note that fake labels are real for generator cost\n",
        "        loss_g = criterion_g(out_labels = output, out_images = fake_sr, target_images = sr) \n",
        "        loss_g.backward()\n",
        "        optimizer_g.step()\n",
        "        running_loss_g += loss_g.item()\n",
        "    discriminator_loss = running_loss_d/dataset_sizes['train']\n",
        "    discriminator_loss_list.append(discriminator_loss)\n",
        "    generator_loss = running_loss_g/dataset_sizes['train']\n",
        "    generator_loss_list.append(generator_loss)\n",
        "\n",
        "    # validation step\n",
        "    model_g.eval()\n",
        "    val_running_loss = 0\n",
        "    torch.set_grad_enabled(False)\n",
        "    for lr, sr in dataloaders['test']:\n",
        "        lr = lr.to(device)\n",
        "        sr = sr.to(device)\n",
        "        fake_sr = model_g(lr)\n",
        "        output = model_d(lr, fake_sr) \n",
        "        loss = criterion_g(out_labels = output, out_images = fake_sr, target_images = sr) \n",
        "        val_running_loss += loss.item() \n",
        "    val_loss = val_running_loss/dataset_sizes['test']\n",
        "    generator_val_loss_list.append(val_loss)\n",
        "    # update the best model\n",
        "    if val_loss < best_loss: \n",
        "        # save the weights\n",
        "        best_loss = val_loss\n",
        "        torch.save(model_g.state_dict(), './model/gan_generator')\n",
        "        torch.save(model_d.state_dict(), './model/gan_discriminator')\n",
        "\n",
        "    # print the progress to determine if the progress has stagnated\n",
        "    print(f'epoch: {epoch+1}/{num_epochs}, Generator Loss: {generator_loss:.4f}, Discriminator Loss: {discriminator_loss:.4f}, Generator Validation Loss: {val_loss:.8f}')"
      ],
      "metadata": {
        "id": "gFhj7nxxGgVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unit test\n",
        "# sample output from pretrained generator\n",
        "model_g.to('cpu')\n",
        "img = next(iter(dataloaders['test']))\n",
        "lr = img[0]\n",
        "gen_sr = model_g(lr)[0]\n",
        "lr = lr[0]\n",
        "sr = img[1][0]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "# Display the LR and HR images using matplotlib\n",
        "axs[0].imshow(ToPILImage()(sr))\n",
        "axs[0].set_title('sr image 1x')\n",
        "axs[1].imshow(ToPILImage()(gen_sr))\n",
        "axs[1].set_title('gen sr image 1x')\n",
        "axs[2].imshow(ToPILImage()(lr))\n",
        "axs[2].set_title('lr image 4x')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SbVEqQEwG984"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adapted from xingyue model\n",
        "\n",
        "# !pip install torchmetrics\n",
        "# !pip install lpips\n",
        "from torchmetrics import PeakSignalNoiseRatio\n",
        "from torchmetrics import StructuralSimilarityIndexMeasure\n",
        "import lpips\n",
        "psnr = PeakSignalNoiseRatio().to(device)\n",
        "ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
        "loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "# Evaluate model performance on test dataset (modified from xinyue model)\n",
        "l1_loss = []\n",
        "mse_loss = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "output_list = []\n",
        "lpips_list = []\n",
        "criterion1 = nn.L1Loss()\n",
        "criterion2 = nn.MSELoss()\n",
        "model_g.to(device)\n",
        "model_g.eval()\n",
        "with torch.no_grad():\n",
        "    for lr, sr in dataloaders['test']:\n",
        "        lr = lr.to(device)\n",
        "        sr = sr.to(device)\n",
        "        fake_sr = model_g(lr)\n",
        "        # Compute L1 loss\n",
        "        loss = criterion1(fake_sr, sr)\n",
        "        l1_loss.append(loss.item())\n",
        "\n",
        "        # Compute MSE loss\n",
        "        loss2 = criterion2(fake_sr, sr)\n",
        "        mse_loss.append(loss2.item())\n",
        "\n",
        "        # Compute PSNR \n",
        "        psnr_value = psnr(fake_sr, sr)\n",
        "        psnr_value = psnr_value.clone().cpu().detach().numpy()\n",
        "        psnr_list.append(psnr_value)\n",
        "        \n",
        "        # Compute SSIM\n",
        "        ssim_value = ssim(fake_sr, sr)\n",
        "        ssim_value = ssim_value.clone().cpu().detach().numpy()\n",
        "        ssim_list.append(ssim_value)\n",
        "\n",
        "        # Compute LPIPS\n",
        "        d = loss_fn.forward(fake_sr, sr).clone().cpu().detach().numpy()\n",
        "        lpips_list.append(d)\n",
        "        \n",
        "print('l1_loss:', np.mean(l1_loss))\n",
        "print('mse_loss', np.mean(mse_loss))\n",
        "print('psnr:', np.mean(psnr_list))\n",
        "print('lpips:', np.mean(lpips_list))\n",
        "print('ssim:', np.mean(ssim_list))"
      ],
      "metadata": {
        "id": "GKCRcAxLfU_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}